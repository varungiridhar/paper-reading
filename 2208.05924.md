# Regularizing Deep Neural Networks with Stochastic Estimators of Hessian Trace

## Overview
Pretty simple method of regularizing DNN using an stochastic estimator of the Hessian trace.
## Method
- Didn't go through exactly what they did with the stochastic estimator, but the algorithm follows an iterative process to approximate the Hessian trace via Hutchinson's method. They also optionally dropout to reduce the number of parameters (speeding up Hessian trace estimation).
- The key motivation behind this is another work that shows that Hessian trace improve generalization in DNNs by the fact flat minima improves generalization and we can "enhance the chance of finding flat minima" by regularizing the Hessian trace.
## Interesting Takeaways
- I liked their approach to optimization process as a nonlinear dynamical system... however one of the reviewer didn't like this, or it wasn't convincing or something... I found it interesting that they opted for the optimizer finding an unstable minima; in this way, it is easier to jump out of a local minima. They connected this to Lyapunov stability and the fact that the Hessian trace regularization can help with this.

## Thoughts
- Very simple idea... though I did not look into it too deeply, it would be interesting to see how they made their stochastic estimators differentiable for the backward pass. I am right now looking into HVP for my own work, so I will probably look into this paper more closely later.