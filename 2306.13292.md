Variance-Covariance Regularization Improves Representation Learning

## Overview
Framework that maximizes variance and minimizes covariance of a learned representation (last linear layer) to improve transfer learning, in particular. Very ML paper, but cool to see some of the parallels between this and policy gradient methods in RL.

## Method
- $\ell_{\text{vcreg}} = \alpha \ell_{\text{var}}(h_1...h_N) + \beta \ell_{\text{cov}}(h_1...h_N)$ where $h_i^N_{i=1}$ is the D-dimensional input representation to the last layer of the network.
- Easily extends to intermediate layers as well, which they found to be useful
- Fast implementation since the VCRegLoss calculation solely relies on the current representation. As a resuult, they can just adjust the gradients...
- They prove their regularization on transfer learning scenarios with images, videos, and class-imbalance with long-tail learning. 
## Interesting Takeaways
- Their approach was very simple and it works well in practice. I think their two moon dataset experiment was a cool toy example to show that their reguarlization method actually helps with better representation learning. They also show neural collapse in terms of 2 metrics. 
## Thoughts
- Not too related to robotics, and in particular RL, since this is the self-supervised learning setting. However, some of their regularization and stabalizing tricks may come in handy...