# Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization

## Overview
Very simple paper on how to improve sample efficiency (number of gradient updates per unit data) utilizing weight normalization to account for instabilities in high UTD ratio for CrossQ method. TL;DR, make crossQ robust to high UTD, leading to better perf and sample efficiency

## Method
- SAC
- Apply WM to the first 2 layers of SAC implementation in CrossQ. Additionally add weight decay

## Interesting Takeaways
- ...

## Thoughts
- I think this was a very simple concept, whose rigor lied more in the testing or their concept.