WorldVLA: Towards Autoregressive Action World Model

## Overview
Combine the concepts of VLA's with world models with the intention of ingraining some physics reasoning / world knowledge into standard VLA models. Their main novelty is to train the world model alongside the VLA, which allows the VLA to "endow system with the capacity for prospective simulation, enabling it to anticipate the consequences of potential actions."
## Method

- $L = L_{action_model} + \alpha L_{world_model}$ (it's as easy as that, though stabilizing this training recipe was probably non-trivial, they don't mention it)
- Model is initialized from Chameleon (Team, 2024) - never heard of it.
- They also mask action data a bit differently (I'm assuming for the action world model training), conditioning next action prediction ONLY on the previous observation and text prompt, not the previous action. This is to avoid the model learning to predict the next action based on the previous action, which is not stable in an autoregressive setting.
![WorldVLA Architecture](/figures/world_vla.png)
## Interesting Takeaways
- Generating multiple action in sequence leads to performance drops due to compounding errors; so they intentionally condition next action only on previous observation and text prompt, not previous actions. They hypothesized this is because MLLM have been exposed to images and text rather than actions, which results in limited action generalization ability.
- World models help action model AND action models help world model. The former is easy to see... the latter is less intuitive. They hypothesize this is because action model derives actions based on input images which leads to more accurate visual interpretation. In other words, whatever the action model spits out will have good understanding of visual and action-related information, leading to better more stable world model rollouts.
- Lastly, they show that we can use a pretrained world model to warm-start the action model training, essentially.

## Thoughts
- I think this paper was very cool and opened up a new avenue to scale large robot models. I'm still a bit skeptical about some of their claims. In particular, action models helping world models, and I'd also like to see some more ablations / theoretical support for whether having this autoregressive world model is actually helping the action model, or if it's just the extra data / regularization effect. Also, they did not really talk about how they stabalized the joint training of the world model and action model, which I think is a non-trivial problem.